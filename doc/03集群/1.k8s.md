## kubernates--K8S

> 官方文档：https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/
>
> 社区文档：https://www.kubernetes.org.cn/k8s

### 1 基本概念

#### 1.1 kubernates是什么？

参照官方文档

#### 1.2 基本组件

1、kubernetes集群的基本组件：

![image-20210808225029210](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210808225029210.png)

1）控制平面组件：Control Plane Components

控制平面的组件对集群做出全局决策(比如调度)，以及检测和响应集群事件（例如，当不满足部署的 `replicas` 字段时，启动新的 `pod`。控制平面组件可以在集群中的任何节点上运行。 

- kube-apiserver：API 服务器是 Kubernetes 控制面的组件， 该组件公开了Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。
- etcd：etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。
- kube-scheduler：控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。
- kube-controller-manager：运行控制器进程的控制平面组件。从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。
  - 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应
  - 任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成
  - 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod)
  - 服务帐户和令牌控制器（Service Account & Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌
- cloud-controller-manager：云控制器管理器是指嵌入特定云的控制逻辑的 控制平面组件。 云控制器管理器使得你可以将你的集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与你的集群交互的组件分离开来。`cloud-controller-manager` 仅运行特定于云平台的控制回路。 如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的环境中不需要云控制器管理器。

2）Node组件

节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。

- kubelet：一个在集群中每个节点（node）上运行的代理。 它保证容器（containers）都 运行在 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 中。kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。
- kube-proxy：kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。
- Container Runtime：容器运行环境是负责运行容器的软件。Kubernetes 支持多个容器运行环境: Docker、 containerd、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口)。

- 插件（Addons）：插件使用 Kubernetes 资源（DaemonSet、 Deployment等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 `kube-system` 命名空间。

2、访问流程

![image-20210808225240911](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210808225240911.png)

### 2 部署

#### 2.1 集群部署

##### 2.1.1 流程分析

- 所有节点安装Docker+kubeadm。
- 部署kubernetes Master。
- 部署容器网络插件。
- 部署Kubernetes Node，将节点加入Kubernetes集群中。
- 部署Dashboard Web页面，可视化查看Kubernetes资源。

![image-20210808231404335](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210808231404335.png)

##### 2.1.2 环境准备

1、准备3台虚拟机

2、配置静态ip地址

>192.168.134.181(master)
>
>192.168.134.182
>
>192.168.134.183

3、可以联通外网

4、关闭内存交换

- 查看swap文件信息：`cat /etc/fstab`
- 临时关闭内存交换：`swapoff -a`
- 永久关闭内存交换：`sed -ri 's/.*swap.*/#&/' /etc/fstab`

![image-20210808235944420](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210808235944420.png)

5、配置hostname

vim /ect/hostname（重启生效）或者hostnamectl set-hostname <newhostname>

- k8s-guli01
- k8s-guli02
- k8s-guli03

6、配置地址映射：保证可以使用hostname ping的通

> vim /etc/hosts

```shell
#服务器中都添加如下内容
192.168.134.181 k8s-guli01
192.168.134.182 k8s-guli02
192.168.134.183 k8s-guli03
```

7、将ipv4流量传递到iptables的链

```shell
[root@k8s-guli01 ~]# cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
```

应用配置的规则：`sysctl --system`

![image-20210809001927284](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809001927284.png)

##### 2.1.3 安装k8s集群

> Kubernetes默认的CRI(容器运行时)为Docker，因此先安装Docker。

1、安装docker

> 参照官方文档：https://docs.docker.com/engine/install/centos/

注意因为kubernates依赖于docker，因此需要设置docker开机启动`systemctl enable docker`。

2、添加阿里云yum源

```shell
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

3、安装kubeadm,kubelet,kubectl

```shell
#检查
yum list|grep kube
#安装
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3
#开机启动
systemctl enable kubelet
systemctl start kubelet
#检查启动状态：启动不起来是很正常的，还没有配置结束
systemctl enable kubelet
```

4、初始化master节点

下载镜像：

```shell
#!/bin/bash

images=(
	kube-apiserver:v1.17.3
    kube-proxy:v1.17.3
	kube-controller-manager:v1.17.3
	kube-scheduler:v1.17.3
	coredns:1.6.5
	etcd:3.4.3-0
    pause:3.1
)

for imageName in ${images[@]} ; do
    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
#   docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName  k8s.gcr.io/$imageName
done

```

初始化master：

```shell
kubeadm init \
--apiserver-advertise-address=192.168.134.181 \
--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
--kubernetes-version v1.17.3 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=10.244.0.0/16
```

成功标志：

```shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.134.181:6443 --token nkdg4n.jwx59bgz63fcv08c \
    --discovery-token-ca-cert-hash sha256:a4dd6ba70d07e4546b0c42b0f5c4d822ecd7b6c05e9926709634325efb59ca90 
```

![image-20210809011711496](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809011711496.png)

根据提示执行：

```shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

配置pod网络(CNI)：

- 方式1：在线下载

  ```shell
  kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
  ```

- 方式2：使用本地文件`kube-flannel.yml`

  ```shell
  kubectl apply -f kube-flannel.yml
  ```

查看pods：kubectl get pods --all-namespaces

> 一定要让kube-flannel处于运行状态

![image-20210809013057861](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809013057861.png)

获取kubectl的nodes：`kubectl get nodes`

![image-20210809013232388](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809013232388.png)

将其他节点加入到master节点中：

```shell
kubeadm join 192.168.134.181:6443 --token nkdg4n.jwx59bgz63fcv08c \
    --discovery-token-ca-cert-hash sha256:a4dd6ba70d07e4546b0c42b0f5c4d822ecd7b6c05e9926709634325efb59ca90
```

成功标志：

```shell
This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

再次查看nodes信息：`kubectl get nodes`

![image-20210809013514730](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809013514730.png)

监视pod进度：`watch kubectl get pod -n kube-system -o wide`

![image-20210809013735318](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809013735318.png)

等到所有状态都是running，再次查看nodes：`kubectl get nodes`

![image-20210809013804457](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809013804457.png)

### 3 基本操作

#### 3.1 部署tomcat

> 官方文档：http://docs.kubernetes.org.cn/683.html
>
> 注意：在主节点上执行。

1、部署tomcat

创建tomcat：`kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8`执行结果：deployment.apps/tomcat6 created

获取所有资源不同的对象：`kubectl get all -o wide`

![image-20210809211542173](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809211542173.png)

我们可以发现在node3上已经ready，兵器已经启动起来，因此我们可以去node3上进行验证。

![image-20210809211717872](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809211717872.png)

获取所有的pod的信息：`kubectl get pods -o wide`

![image-20210809212147529](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809212147529.png)

假设关掉node3机器，再次查看pods信息，我们可以发现node3上tomcat被终端，node2节点上又被启动了一份。

2、暴露对外访问的端口

```shell
#堆外暴露端口，port是80，容器端口是8080，会自动生成一个service端口。
kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort
```

![image-20210809213851350](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809213851350.png)

访问：http://192.168.134.181:31551/

3、扩容

```shell
kubectl scale --replicas=3 deployment tomcat6
```

4、缩容

```shell
kubectl scale --replicas=1 deployment tomcat6
```

5、删除部署信息

```shell
kubectl delete deployment.apps/tomcat6
kubectl delete service/tomcat6
```

6、使用yaml来创建pod

创建yaml文件：

```shell
kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --dry-run -o yaml > tomcat6.yaml
```

tomcat6.yaml内容：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tomcat6
  name: tomcat6
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tomcat6
  template:
    metadata:
      labels:
        app: tomcat6
    spec:
      containers:
      - image: tomcat:6.0.53-jre8
        name: tomcat
```

使用yaml来创建pod：`kubectl apply -f tomcat6.yaml`

7、使用yaml来暴露服务

```shell
[root@k8s-guli01 ~]# kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml

yaml：
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: tomcat6
  name: tomcat6
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat6
  type: NodePort
status:
  loadBalancer: {}
```

#### 3.2 使用yaml配置

1、创建yaml的模板

```shell
kubectl create deployment tomcat6 --image=tomcat:6.0.53-jre8 --dry-run -o yaml > tomcat6-deployment.yaml

[root@k8s-guli01 ~]# cat tomcat6-deployment.yaml
#修改内容如下所示：
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tomcat6
  name: tomcat6
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tomcat6
  strategy: {}
  template:
    metadata:
      labels:
        app: tomcat6
    spec:
      containers:
      - image: tomcat:6.0.53-jre8
        name: tomcat
```

2、部署pod

```shell
kubectl apply -f tomcat6-deployment.yaml
```

3、合并部署和暴露端口yaml文件

```shell
#尝试暴露端口，将内容输出到export.yaml文件中
kubectl expose deployment tomcat6 --port=80 --target-port=8080 --type=NodePort --dry-run -o yaml > export.yaml

#查看暴露端口的yaml
[root@k8s-guli01 ~]# cat export.yaml 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: tomcat6
  name: tomcat6
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat6
  type: NodePort
status:
  loadBalancer: {}

#合并部署和暴露端口的yaml信息
[root@k8s-guli01 ~]# cat tomcat6-deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tomcat6
  name: tomcat6
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tomcat6
  strategy: {}
  template:
    metadata:
      labels:
        app: tomcat6
    spec:
      containers:
      - image: tomcat:6.0.53-jre8
        name: tomcat
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: tomcat6
  name: tomcat6
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: tomcat6
  type: NodePort

#部署暴露端口一起执行
[root@k8s-guli01 ~]# kubectl apply -f tomcat6-deployment.yaml 
deployment.apps/tomcat6 created
service/tomcat6 created
```

![image-20210809225028850](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809225028850.png)

#### 3.3 安装ingress 

> 通过Service发现pod进行关联，基于域名访问。
>
> 用ingress Controller实现pod的负载均衡。
>
> 支持TCP/UDP4层不在均衡和HTTP 7层负载均衡。

![image-20210809225253134](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210809225253134.png)

1、yaml安装ingress controller

```yaml
[root@k8s-guli01 k8s]# cat ingress-controller.yaml 
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: DaemonSet 
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      hostNetwork: true
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: siriuszg/nginx-ingress-controller:0.20.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
spec:
  #type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP
  - name: https
    port: 443
    targetPort: 443
    protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
```

2、配置一个ingress规则

```yaml
[root@k8s-guli01 k8s]# cat ingress-tomcat6.yaml 
apiVersion: extensions/v1beta1
kind: Ingress 
metadata: 
  name: web 
spec:
  rules:
  - host: tomcat.guli.com
    http:
      paths:
        - backend:
           serviceName: tomcat6
           servicePort: 80
```

3、配置域名

> C:\Windows\System32\drivers\etc

```txt
192.168.134.181 tomcat.guli.com
```

4、访问测试

### 4 安装KubeSphere

> 官方文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/installation/prerequisites/

#### 4.1 前置环境

##### 4.1.1 安装helm&tiller

安装helm&tiller

1）在线安装

> 注意：helm3移除tiller，因此我们可以下载helm2，如下方式下载是最新版

```shell
[root@k8s-guli01 k8s]# curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scri
[root@k8s-guli01 k8s]# ./get_helm.sh
Downloading https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz
Verifying checksum... Done.
Preparing to install helm into /usr/local/bin
helm installed into /usr/local/bin/helm
```

2）离线安装：

> 下载合适版本：https://github.com/helm/helm/releases/tag/v2.16.3
>
> 解压文件：tar -zxvf helm-v2.16.3-linux-amd64.tar.gz
>
> 将helm和tiller复制到bin目录下：
>
> cd linux-amd64
>
> cp helm /usr/local/bin/
>
> cp tiller /usr/local/bin/
>
> 查看版本helm version

2、准备`helm_rbac.yaml`文件

```yaml
[root@k8s-guli01 k8s]# cat helm_rbac.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
```

3、应用`helm_rbac.yaml`

```shell
[root@k8s-guli01 k8s]# kubectl apply -f helm_rbac.yaml 
serviceaccount/tiller created
clusterrolebinding.rbac.authorization.k8s.io/tiller created
```

4、初始化

```shell
[root@k8s-guli01 k8s]# helm init --service-account tiller --tiller-image=sapcc/tiller:v2.16.3 --history-max 300
Creating /root/.helm 
Creating /root/.helm/repository 
Creating /root/.helm/repository/cache 
Creating /root/.helm/repository/local 
Creating /root/.helm/plugins 
Creating /root/.helm/starters 
Creating /root/.helm/cache/archive 
Creating /root/.helm/repository/repositories.yaml 
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com 
Error: error initializing: Looks like "https://kubernetes-charts.storage.googleapis.com" is not a valid chart repository or cannot be reached: Failed to fetch https://kubernetes-charts.storage.googleapis.com/index.yaml : 403 Forbidden
```

出现错误：Error: error initializing: Looks like "https://kubernetes-charts.storage.googleapis.com" is not a valid chart repository or cannot be reached: Failed to fetch https://kubernetes-charts.storage.googleapis.com/index.yaml : 403 Forbidden

解决方式：

```shell
[root@k8s-guli01 k8s]# echo "" > /root/.helm/repository/repositories.yaml
[root@k8s-guli01 k8s]# helm init 
```

过程：

```shell
[root@k8s-guli01 k8s]# helm repo list
Error: Couldn't load repositories file (/root/.helm/repository/repositories.yaml).
You might need to run `helm init` (or `helm init --client-only` if tiller is already installed)
[root@k8s-guli01 k8s]# echo "" > /root/.helm/repository/repositories.yaml
[root@k8s-guli01 k8s]# helm init --service-account tiller --tiller-image=sapcc/tiller:v2.16.3 --history-max 300
Updating repository file format...
$HELM_HOME has been configured at /root/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.

Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.
To prevent this, run `helm init` with the --tiller-tls-verify flag.
For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation
[root@k8s-guli01 k8s]# helm repo list
NAME	URL
```

验证版本：

```shell
[root@k8s-guli01 linux-amd64]# helm version
Client: &version.Version{SemVer:"v2.16.3", GitCommit:"1ee0254c86d4ed6887327dabed7aa7da29d7eb0d", GitTreeState:"clean"}
Server: &version.Version{SemVer:"v2.16.3", GitCommit:"1ee0254c86d4ed6887327dabed7aa7da29d7eb0d", GitTreeState:"clean"}
[root@k8s-guli01 linux-amd64]# tiller version
[main] 2021/08/09 09:59:20 Starting Tiller v2.16.3 (tls=false)
[main] 2021/08/09 09:59:20 GRPC listening on :44134
[main] 2021/08/09 09:59:20 Probes listening on :44135
[main] 2021/08/09 09:59:20 Storage driver is ConfigMap
[main] 2021/08/09 09:59:20 Max history per release is 0
```

验证是否安装到kubenates

```shell
[root@k8s-guli01 linux-amd64]# kubectl get pods --all-namespaces
NAMESPACE       NAME                                 READY   STATUS    RESTARTS   AGE
default         tomcat6-5f7ccf4cb9-5stwd             1/1     Running   1          131m
default         tomcat6-5f7ccf4cb9-9sxcg             1/1     Running   1          131m
default         tomcat6-5f7ccf4cb9-tpw9c             1/1     Running   1          131m
ingress-nginx   nginx-ingress-controller-vms5r       1/1     Running   1          120m
ingress-nginx   nginx-ingress-controller-xb8sl       1/1     Running   1          120m
kube-system     coredns-7f9c544f75-99q24             1/1     Running   2          23h
kube-system     coredns-7f9c544f75-btwmh             1/1     Running   2          23h
kube-system     etcd-k8s-guli01                      1/1     Running   2          23h
kube-system     kube-apiserver-k8s-guli01            1/1     Running   2          23h
kube-system     kube-controller-manager-k8s-guli01   1/1     Running   2          23h
kube-system     kube-flannel-ds-amd64-2phx9          1/1     Running   4          23h
kube-system     kube-flannel-ds-amd64-7wbpz          1/1     Running   3          23h
kube-system     kube-flannel-ds-amd64-n9tvt          1/1     Running   2          23h
kube-system     kube-proxy-pj8mh                     1/1     Running   3          23h
kube-system     kube-proxy-v2tj4                     1/1     Running   3          23h
kube-system     kube-proxy-xpt52                     1/1     Running   2          23h
kube-system     kube-scheduler-k8s-guli01            1/1     Running   2          23h
kube-system     tiller-deploy-5fdc6844fb-mfh2s       1/1     Running   0          11m
```

##### 4.1.2 安装Open EBS

1、获取node信息

```shell
[root@k8s-guli01 linux-amd64]# kubectl get node -o wide
NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION          CONTAINER-RUNTIME
k8s-guli01   Ready    master   23h   v1.17.3   192.168.134.181   <none>        CentOS Linux 7 (Core)   3.10.0-693.el7.x86_64   docker://20.10.8
k8s-guli02   Ready    <none>   23h   v1.17.3   192.168.134.182   <none>        CentOS Linux 7 (Core)   3.10.0-693.el7.x86_64   docker://20.10.8
k8s-guli03   Ready    <none>   23h   v1.17.3   192.168.134.183   <none>        CentOS Linux 7 (Core)   3.10.0-693.el7.x86_64   docker://20.10.8
```

2、查看master节点是否有`Taint`

```shell
[root@k8s-guli01 linux-amd64]# kubectl describe node k8s-guli01 | grep Taint
Taints:             node-role.kubernetes.io/master:NoSchedule
```

3、去掉master的`Taint`：去掉污点

```shell
[root@k8s-guli01 linux-amd64]# kubectl taint nodes k8s-guli01 node-role.kubernetes.io/master:NoSchedule-
node/k8s-guli01 untainted
[root@k8s-guli01 linux-amd64]# kubectl describe node k8s-guli01 | grep Taint
Taints:             <none>
```

4、安装Open EBS

```shell
#创建名称空间
[root@k8s-guli01 linux-amd64]# kubectl create ns openebs
#使用helm安装open ebs（失败）
[root@k8s-guli01 linux-amd64]#  install --namespace openebs --name openebs stable/openebs --version 1.5.0
Error: failed to download "stable/openebs" (hint: running `helm repo update` may help)
#使用yaml的方式安装
[root@k8s-guli01 linux-amd64]# kubectl apply -f https://openebs.github.io/charts/openebs-operator-1.5.0.yaml

```

等待安装完成：使用`kubectl get pods --all-namespaces`查看状态

5、查看`StorageClass`

安装Open EBS之后会自动创建4个`StorageClass`，使用`kubectl get sc`查看创建StorageClass

![image-20210810204223977](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210810204223977.png)

6、设置默认的`StorageClass`

```shell
[root@k8s-guli01 ~]# kubectl patch storageclass openebs-hostpath -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

storageclass.storage.k8s.io/openebs-hostpath patched
```

![image-20210810204506402](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210810204506402.png)

7、将master节点打上污点

因为之前去掉了master节点的Taint，我们再安装完KubeSphere之后，可以将Taint重新加上，避免业务相关的工作负载调度到master节点上抢占master的资源。

```shell
[root@k8s-guli01 ~]# kubectl taint nodes k8s-guli01 node-role.kubernetes.io/master=:NoSchedule

node/k8s-guli01 tainted
```

##### 4.1.3 安装KubeSphere

> 因为内存的不足，使用最小安装。

1、下载最小化安装文件`kubesphere-minimal.yaml` 

2、使用yaml文件安装

```shell
[root@k8s-guli01 k8s]# kubectl apply -f kubesphere-minimal.yaml 
namespace/kubesphere-system created
configmap/ks-installer created
serviceaccount/ks-installer created
clusterrole.rbac.authorization.k8s.io/ks-installer created
clusterrolebinding.rbac.authorization.k8s.io/ks-installer created
deployment.apps/ks-installer created
```

3、监控log

```shell
[root@k8s-guli01 k8s]# kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f
```

4、访问测试

通过监视日志获取我们的dashboard地址，用户名，密码等信息。

```shell
Start installing monitoring
**************************************************
task monitoring status is successful
total: 1     completed:1
**************************************************
#####################################################
###              Welcome to KubeSphere!           ###
#####################################################

Console: http://192.168.134.181:30880
Account: admin
Password: P@88w0rd

NOTES：
  1. After logging into the console, please check the
     monitoring status of service components in
     the "Cluster Status". If the service is not
     ready, please wait patiently. You can start
     to use when all components are ready.
  2. Please modify the default password after login.

#####################################################
```

> 可以更改自己的密码，我这里改为Code1997

##### 4.1.4 安装devOPS以及告警服务

> 安装后开启DevOps系统

1、修改ks-installer的configmap

```shell
[root@k8s-guli01 k8s]# kubectl edit cm -n kubesphere-system ks-installer

configmap/ks-installer edited
#我们可以打开devOps，sonarqube，日志告警
```

![image-20210811012611362](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210811012611362.png)

### 5 插件的初体验

> 官方文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/quick-start-guide/

#### 5.1 多租户管理

> 官方文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/admin-quick-start/
>
> guli-hr拥有users-manager角色

![image-20210811212951622](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210811212951622.png)

![image-20210811213234929](https://blog-images-code1997.oss-cn-hangzhou.aliyuncs.com/java/project/gmall/03architecture/image-20210811213234929.png)

#### 5.2 创建Wordpree应用

>官方文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/wordpress-deployment/
>
>环境变量，挂在路径等信息以docker hub中的镜像介绍为准。

#### 5.3 基于springboot构建流水线

> 官方文档：https://v2-1.docs.kubesphere.io/docs/zh-CN/quick-start/devops-online/

```shell
mvn sonar:sonar \
  -Dsonar.host.url=http://192.168.134.181:30481 \
  -Dsonar.login=a883857582ea22e424442181a7a829c8b318d3f1
```

